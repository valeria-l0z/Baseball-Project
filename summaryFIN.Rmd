---
title: "126 Data Project, Summary"
date: "Sam Ream, Valeria Lopez, Skyler Yee"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
# knit options
knitr::opts_chunk$set(echo = F,
                      results = 'markup',
                      fig.width = 4,
                      fig.height = 3,
                      fig.align = 'center',
                      message = F,
                      warning = F)

# packages
library(tidyverse)
library(faraway)
library(RSQLite)
library(skimr)
library(GGally)
library(tidymodels)
library(leaps)
library(glmnet)
```

```{r, echo=FALSE}
ball <- dbConnect(drv=RSQLite::SQLite(), dbname="../../data/database.sqlite")
```

```{r, echo =FALSE}
batting <- dbGetQuery(ball, "
                            SELECT
                            sum(ab) AS AT_BAT,
                            player_id,
                            sum(r) AS RUNS,
                            sum(hr) AS HOME_RUNS, 
                            sum(triple) AS TRIPLE,
                            sum(double) AS DOUBLE,
                            (sum(h) -  sum(hr) - sum(triple) - sum(double)) AS SINGLES,
                            sum(bb) AS WALKS,
                            sum(ibb) AS INT_WALKS,
                            sum(sb) AS STOLEN_BASES,
                            sum(hbp) AS HIT_BY_PITCH
                            
                            FROM
                            batting
                            where
                            year > 2000
                            group by
                            player_id
                            
                   ")
sumbat <- batting
batting <- subset(batting,batting[,1]>100)


for (x in 3:11) 
{
  batting[,x] <- batting[,x] / 1
}

set.seed(5)
batting <- sample_n(batting, 500, replace = FALSE)

players <- dbGetQuery(ball, "
                      
                      SELECT 
                      player_id,
                      (weight / POWER(height, 2)) *703 AS BMI,
                      bats as HAND
                      FROM
                      player
                      
                      ")




players <- subset(players, players[,2]>0)



for ( x in 1:17918)
{
  if(players[x,2] <= 18.5) {players[x,2] <- "U"} 
  else if(players[x,2] <= 24.9) {players[x,2] <- "H"}
  else if(players[x,2] <= 29.9) {players[x,2] <- "O"}
  else {players[x,2] <- "B"}
}

batting <-  merge(batting, players, by="player_id" )
```

# Step 1

We selected “The History of Baseball” data set, which is a record of all the statistics of baseball players who have played in the MLB up until 2015. We sampled 500 players who played between 2000-2015 to capture the current state of the game. Our population is all of the MLB players from 2000-2015. We did not want to generalize the whole history of the MLB because the rules of the game and the strategies used by teams and players are always changing. Our predictors are singles, doubles, triples, home runs, walks, stolen bases, hit by pitches, intentional walks, BMI, and batting hand. Our objective was to predict the expected number of runs that future players will have on record, given observations from their past performances.


```{r, eval = FALSE, fig.width=10, fig.height=10}
ggpairs(batting, columns = 3:13)
```
Almost all of our predictors were highly positively correlated to one another. Additionally, most of our predictors were highly correlated to our response variable, Runs. 

# Step 2-3

```{r}
library(lmtest)
DOUBLE <- batting$DOUBLE
g <- lm(batting$RUNS~DOUBLE ,data=batting)
dwtest(batting$RUNS ~ batting$DOUBLE, data=batting)
```


```{r fig.cap= "Scatter plot of the relationship between Runs and Doubles"}
par(mfrow = c(1, 2))
plot(batting$DOUBLE, batting$RUNS, ylab = "Runs", xlab = "Doubles")
abline(0,2.4, col="red",)
abline(-150,2.4, col="blue",)
abline(200,2.4, col="blue",)
```

```{r fig.cap= "QQ-Plot showing that the data is not normally distributed"}
qqnorm(batting$RUNS)
qqline(batting$RUNS)
```

**- Linearity:** All the points on the relationship plot above are arranged in a very linear way without transformations

**- Constant Variance:** Almost all of the points have a similar distance from a proposed straight line.

**- Independence:** With the knowledge that one batter hitting the ball well enough to get a double does not affect the likelihood of the next batter doing the same, we know that the predictors are independent of one another.

**- Normality:** While our errors do not appear to be normally distributed, our large sample size allows us to leverage the Central Limit Theorem to make meaningful analysis.

```{r include=FALSE}
slm <- summary(g)
anova_lm <- anova(g)
```

# Hypothesis Testing

## Significance Test

$$H_0: \beta_i=0$$

$$H_a: \beta_1 \neq 0$$

$$\alpha = 0.05$$

```{r include=FALSE}
test_stat <- slm$coefficients[,3][2]
test_stat #102.942 # from slm
p_value <- slm$coefficients[,4][2]
p_value
```

$\textbf{Test Statistic =}\ 102.942$

$\textbf{P Value} \approx 0$

We reject $H_0$ at 0.05 level. Thus, the amount of doubles a player hits is a significant predictor of how many runs the player scores.

# Fit of Model

The $R^2$ value of our model is 0.9551, which means that the model explains 95.51% of the variance of the recorded events. Additionally, the residual plot in figure 3 shows how the data points share a similar spread which implies that the model is a good fit.

```{r include=FALSE}
summary(lm(batting$DOUBLE ~ batting$RUNS))
```

```{r fig.cap= "Residual Plot of the fitted model"}
model <- lm(batting$RUNS ~ batting$DOUBLE)

res <- resid(model)

plot(fitted(model), res, ylab = "Residuals", xlab = "Fitted Model")
```

# Computational Models

For our computational models, we used the predictors: Total Intentional Walks, Singles, Triples, Stolen Bases, and Home Runs obtained in a career. We selected these predictors because of their low correlation in addition to their interesting relation to obtained Runs.

### Model 1 - Full Model ( $\Omega$ )

$\mathbb{E}[Y]=\text{Intercept } + \text{Intentional Walks }+\text{Singles }+\text{Triples }+\text{Stolen Bases }+\text{Home Runs}+\epsilon$

### Model 2 - Reduced Model ( $\omega$ )

$\mathbb{E}[Y]=\text{Intercept } + \text{Singles }+\text{Triples }+\text{Home Runs}+\epsilon$

### Comparison:

$H_0: \beta \in \omega :\text{The Reduced Model is sufficient}$

$H_\alpha: \beta \in \Omega \text{\\} \omega \in w :\text{The Reduced Model is not sufficient}$

```{r}
model_1 <- lm(RUNS~INT_WALKS + SINGLES + TRIPLE + STOLEN_BASES + HOME_RUNS, data = batting)
model_2 <- lm(RUNS~SINGLES + TRIPLE + HOME_RUNS, data = batting)
anova(model_1, model_2)
```

### Conclusion

As we rejected \$H_0\$ in favor for \$H\_\\alpha\$, we can determine that the reduced model does not model the data well enough to justify the reduction in predictors. As such, we decided to use model 1, the full model, as our computational model.

# Statistical Model

We used a stepwise search to create the best model for our data. For a size of 4 predictors the variables home runs, singles, walks, and stolen bases create a well fit model.

# Final Model Selection

Between the two models we created, the statistical model and computational model, we selected the statistical model. The reason behind this selection is that the statistical model has a larger $R_{adj}^2$ value and we want to explain as much of the variance as possible in our model.


# Step 4

```{r}
tempco <- round(cor(batting[,c('TRIPLE', 'HOME_RUNS', 'SINGLES', 'WALKS', 'DOUBLE', 'INT_WALKS', 'STOLEN_BASES', 'HIT_BY_PITCH')]), 2)
ik <- 1
```

### Collinearity table

|                    |                T |               HR |                S |                W |                D |               IW |               SB |                             HBP |
|:-------|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|
| TRIPLE (T)        | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| HOME_RUNS (HR)     | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| SINGLES (S)        | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| WALKS (W)          | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| DOUBLE (D)         | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| INT_WALKS (IW)     | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| STOLEN_BASES (SB)  | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| HIT_BY_PITCH (HBP) | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |

As we observed the collinearity between certain predictors to be high as seen in the table above, we implemented a Ridge and Lasso regression on the entire set of predictors to make a model that fits well and utilizes all predictors. While the Lasso regression did not eliminate any predictors, it did result in the best fit of all the previous models as can be demonstrated in the plot and table below.

```{r, echo =FALSE}


y <- batting$RUNS
x <- data.matrix(batting[, c('TRIPLE', 'HOME_RUNS', 'SINGLES', 'WALKS', 'DOUBLE', 'INT_WALKS', 'STOLEN_BASES', 'HIT_BY_PITCH')])

stat_model <- lm(RUNS~HOME_RUNS + SINGLES + WALKS + STOLEN_BASES, data = batting)
slm <- lm(RUNS~DOUBLE, data = batting)

cv_model <- cv.glmnet(x, y, alpha = 1)
model <- glmnet(x, y, alpha = 1)

cv_model_ridge <- cv.glmnet(x, y, alpha = 0)
ridge_model <- glmnet(x, y, alpha = 0)
model <- glmnet(x, y, alpha = 1)


best_lambda_r <- cv_model_ridge$lambda.min
best_lambda <- cv_model$lambda.min






```




Coefficients of the Ridge Model
```{r}
temp_ridge <- cv.glmnet(x, y, alpha = 0)
coef(temp_ridge)
```

In observing our coefficients, we can see that Triples have the largest effect (an increase of 1.9420 expected runs per Triple) on the expected number of Runs and that every predictor contributes some information to the model.

Coefficients of the Lasso Model
```{r}
temp_lasso <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(temp_lasso)
```

Through observing our coefficients, we can observe that Triples and Home Runs have a much larger effect on the expected number of Runs than any other predictor in a lasso regression model. Intentional walks also decreases the expected number of runs and no predictor can be removed without consequence.


```{r}
batting <- dbGetQuery(ball, "
                            SELECT
                            sum(ab) AS AT_BAT,
                            player_id,
                            sum(r) AS RUNS,
                            sum(hr) AS HOME_RUNS, 
                            sum(triple) AS TRIPLE,
                            sum(double) AS DOUBLE,
                            (sum(h) -  sum(hr) - sum(triple) - sum(double)) AS SINGLES,
                            sum(bb) AS WALKS,
                            sum(ibb) AS INT_WALKS,
                            sum(sb) AS STOLEN_BASES,
                            sum(hbp) AS HIT_BY_PITCH
                            
                            FROM
                            batting
                            where
                            year > 2000
                            group by
                            player_id
                            
                   ")
sumbat <- batting
batting <- subset(batting,batting[,1]>100)

set.seed(4)
batting <- sample_n(batting, 500, replace = FALSE)

players <- dbGetQuery(ball, "
                      
                      SELECT 
                      player_id,
                      (weight / POWER(height, 2)) *703 AS BMI,
                      bats as HAND
                      FROM
                      player
                      
                      ")




players <- subset(players, players[,2]>0)



for ( x in 1:17918)
{
  if(players[x,2] <= 18.5) {players[x,2] <- "U"} 
  else if(players[x,2] <= 24.9) {players[x,2] <- "H"}
  else if(players[x,2] <= 29.9) {players[x,2] <- "O"}
  else {players[x,2] <- "B"}
}

batting <-  merge(batting, players, by="player_id" )
x <- data.matrix(batting[, c('TRIPLE', 'HOME_RUNS', 'SINGLES', 'WALKS', 'DOUBLE', 'INT_WALKS', 'STOLEN_BASES', 'HIT_BY_PITCH')])

ridge_model_y_predictions <- data.frame(Prediction = 1:500, Actual= 1:500)
ridge_model_y_predictions[,1] <- predict(ridge_model, s = best_lambda_r, newx = x)
ridge_model_y_predictions[,2] <- batting$RUNS

lasso_model_y_predictions <- data.frame(Prediction = 1:500, Actual = 1:500)
lasso_model_y_predictions[,1] <- predict(cv_model, s = best_lambda, newx = x)
lasso_model_y_predictions[,2] <- batting$RUNS


stat_model_y_predictions <- data.frame(Prediction = 1:500, Actual = 1:500)
stat_model_y_predictions[,1] <- predict(stat_model, newdata = batting)
stat_model_y_predictions[,2] <- batting$RUNS

slm_y_predictions <- data.frame(Prediction = 1:500, Actual = 1:500)
slm_y_predictions[,1] <- predict(slm, newdata = batting)
slm_y_predictions[,2] <- batting$RUNS

```







```{r, fig.width=7,fig.height=2, fig.cap="A comparison of the Linear Models"}
ggplot() + 
  
  geom_point(aes(x=Prediction, y=Actual, color="SLM"), slm_y_predictions, size =0.25 )+
  geom_smooth(aes(x=Prediction, y=Actual, color="SLM"), slm_y_predictions, method = lm, fullrange =TRUE, size = 0.75)+
  
  geom_point(aes(x=Prediction, y=Actual, color="MLR"), stat_model_y_predictions,size = 0.25) + 
  geom_smooth(aes(x=Prediction, y=Actual, color="MLR"), stat_model_y_predictions, method = lm, fullrange =TRUE, size = 0.75)+
  
  geom_point(aes(x=Prediction, y=Actual, color="Ridge Model"), ridge_model_y_predictions, size =0.25)+
  geom_smooth(aes(x=Prediction, y=Actual, color="Ridge Model"), ridge_model_y_predictions, method = lm, fullrange =TRUE, size = 0.75)+
  
  geom_point(aes(x=Prediction, y=Actual, color="Lasso Model"), lasso_model_y_predictions, size =0.25 )+
  geom_smooth(aes(x=Prediction, y=Actual, color="Lasso Model"), lasso_model_y_predictions, method = lm, fullrange =TRUE, size = 0.75)+
  
  labs(color = "Model")
```

| Model Type       |                                                                                                                                                      $R^2$ |                                                                                MSE |
|:------------------------------------------|--------------:|--------------:|
| SLR              |                                 `r 1-(sum((slm_y_predictions[,1]-slm_y_predictions[,2])^2)/sum((slm_y_predictions[,2]-sum(slm_y_predictions[,2])/500)^2))` |                 `r (1/500) * sum((slm_y_predictions[,2]-slm_y_predictions[,1])^2)` |
| MLR              |     `r 1-(sum((stat_model_y_predictions[,1]-stat_model_y_predictions[,2])^2)/sum((stat_model_y_predictions[,2]-sum(stat_model_y_predictions[,2])/500)^2))` |   `r (1/500) * sum((stat_model_y_predictions[,2]-stat_model_y_predictions[,1])^2)` |
| Ridge Regression | `r 1-(sum((ridge_model_y_predictions[,1]-ridge_model_y_predictions[,2])^2)/sum((ridge_model_y_predictions[,2]-sum(ridge_model_y_predictions[,2])/500)^2))` | `r (1/500) * sum((ridge_model_y_predictions[,2]-ridge_model_y_predictions[,1])^2)` |
| Lasso Regression | `r 1-(sum((lasso_model_y_predictions[,1]-lasso_model_y_predictions[,2])^2)/sum((lasso_model_y_predictions[,2]-sum(lasso_model_y_predictions[,2])/500)^2))` | `r (1/500) * sum((lasso_model_y_predictions[,2]-lasso_model_y_predictions[,1])^2)` |

### Analysis

As some predictors had a very high level of collinearity, we elected to use shrinkage methods to fit a model to all predictors. The graph and table above were generated for several different sets of 500 new random observations from our original data set. In each case, the patterns displayed were consistent with those show above. The Lasso Regression has the lowest Mean Square Error and the Highest $R^2$ of the models--with the Ridge Regression close behind. On the other hand, the Singular linear regression has the lowest $R^2$ and MSE. The Multi-Linear Regression has a slightly lower $R^2$ and MSE than the Lasso and Ridge Regressions. However, it is very close to the shrinkage models and fits the data well--so it might be a good idea to use it if ease of explainability is important.
