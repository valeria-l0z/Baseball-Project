---
title: "126 Data Project, Step 4"
date: "Sam Ream, Valeria Lopez, Skyler Yee"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
# knit options
knitr::opts_chunk$set(echo = F,
                      results = 'markup',
                      fig.width = 4,
                      fig.height = 3,
                      fig.align = 'center',
                      message = F,
                      warning = F)

# packages
library(tidyverse)
library(faraway)
library(RSQLite)
library(skimr)
library(GGally)
library(tidymodels)
library(leaps)
library(glmnet)
library(gridExtra)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
```

```{r, echo=FALSE}
ball <- dbConnect(drv=RSQLite::SQLite(), dbname="../../data/database.sqlite")
```

```{r, echo =FALSE}
batting <- dbGetQuery(ball, "
                            SELECT
                            sum(ab) AS AT_BAT,
                            player_id,
                            sum(r) AS RUNS,
                            sum(hr) AS HOME_RUNS, 
                            sum(triple) AS TRIPLE,
                            sum(double) AS DOUBLE,
                            (sum(h) -  sum(hr) - sum(triple) - sum(double)) AS SINGLES,
                            sum(bb) AS WALKS,
                            sum(ibb) AS INT_WALKS,
                            sum(sb) AS STOLEN_BASES,
                            sum(hbp) AS HIT_BY_PITCH
                            
                            FROM
                            batting
                            where
                            year > 2000
                            group by
                            player_id
                            
                   ")
sumbat <- batting
batting <- subset(batting,batting[,1]>100)

set.seed(5)
batting <- sample_n(batting, 500, replace = FALSE)

players <- dbGetQuery(ball, "
                      
                      SELECT 
                      player_id,
                      (weight / POWER(height, 2)) *703 AS BMI,
                      bats as HAND
                      FROM
                      player
                      
                      ")




players <- subset(players, players[,2]>0)



for ( x in 1:17918)
{
  if(players[x,2] <= 18.5) {players[x,2] <- "U"} 
  else if(players[x,2] <= 24.9) {players[x,2] <- "H"}
  else if(players[x,2] <= 29.9) {players[x,2] <- "O"}
  else {players[x,2] <- "B"}
}

batting <-  merge(batting, players, by="player_id" )
```

```{r}
# x and y setup with current dataset
y <- batting$RUNS
x <- data.matrix(batting[, c('TRIPLE', 'HOME_RUNS', 'SINGLES', 'WALKS', 'DOUBLE', 'INT_WALKS', 'STOLEN_BASES', 'HIT_BY_PITCH')])
```

```{r echo = FALSE}
stat_model <- lm(RUNS~HOME_RUNS + SINGLES + WALKS + STOLEN_BASES, data = batting)
slm <- lm(RUNS~DOUBLE, data = batting)

```

# Introduction

Using the "History of Baseball" data set, we analyzed how our predictors (singles, doubles, triples, home runs, walks, intentional walks, hit by pitches, stolen bases, BMI, and batting hand) affected the runs scored by individual players. We sampled player statistics randomly from games played between 2000-2015, which allowed us to get an accurate representation of the population of all players who played between those years. Using both Ridge Regression and LASSO, we shrunk the size of some predictors to obtain estimates with smaller variance for higher precision.

# Colinearity

### Correlation Table

```{r}
tempco <- round(cor(batting[,c('TRIPLE', 'HOME_RUNS', 'SINGLES', 'WALKS', 'DOUBLE', 'INT_WALKS', 'STOLEN_BASES', 'HIT_BY_PITCH')]), 2)
ik <- 1
```

|                    |                T |               HR |                S |                W |                D |               IW |               SB |                             HBP |
|:-------|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|
| TRIPLE (T)        | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| HOME_RUNS (HR)     | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| SINGLES (S)        | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| WALKS (W)          | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| DOUBLE (D)         | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| INT_WALKS (IW)     | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| STOLEN_BASES (SB)  | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |
| HIT_BY_PITCH (HBP) | `r tempco[ik,1]` | `r tempco[ik,2]` | `r tempco[ik,3]` | `r tempco[ik,4]` | `r tempco[ik,5]` | `r tempco[ik,6]` | `r tempco[ik,7]` | `r tempco[ik,8]` `r ik <- ik+1` |

### Variance Inflation factor Table

```{r}
modmat <- model.matrix(lm(RUNS~TRIPLE + HOME_RUNS + SINGLES + WALKS + DOUBLE + INT_WALKS + STOLEN_BASES + HIT_BY_PITCH, data=batting))
vif(modmat)
```

## Analysis

When looking at our complete set of predictors, Doubles and Singles are highly correlated with one another and also have the largest effect on the variance of our model by a significant margin. As a result, we concluded that they can not be included in the model of all predictors unless we are willing to sacrifice the fit and predictive accuracy of the model. However, If we wanted to train a model that uses all of the predictors, we would want to use a shrinkage method such as a Ridge or Lasso regression.

# Ridge Regression

```{r echo = FALSE}
ridge_model <- glmnet(x, y, alpha = 0)
```

### Optimal Lambda - Ridge Regression

```{r, fig.cap="The relationship between MSE and Log(lambda)"}
cv_model_ridge <- cv.glmnet(x, y, alpha = 0)

# find optimal lambda value that minimizes test MSE
best_lambda_r <- cv_model_ridge$lambda.min

# produce plot of test MSE by lambda value
plot(cv_model_ridge)

```

We found that the MSE was minimized when $\lambda$ is equal to: `r best_lambda_r`

## Model Analysis

#### R-Squared Analysis

```{r}
#calculate R-squared
y_predicted <- predict(ridge_model, s = best_lambda_r, newx =x)

#SST and SSE
sst <- sum((y-mean(y))^2)
sse <- sum((y_predicted - y)^2)

rsq <- 1-(sse/sst)
```

When Lambda equals `r best_lambda_r`, the R-Squared is `r rsq`. This implies that the model explains approximately `r round(rsq*100,2)`% of the variation in the response in our training data set.

#### Coefficient Analysis



```{r}
temp_ridge <- cv.glmnet(x, y, alpha = 0)
coef(temp_ridge)
```

In observing our coefficients, we can see that Triples have the largest effect (an increase of 1.9420 expected runs per Triple) on the expected number of Runs and that every predictor contributes some information to the model.

# Lasso Regression

```{r echo = FALSE}
model <- glmnet(x, y, alpha = 1)
```

### Optimal Lambda - Lasso Regression

```{r, fig.cap="The relationship between MSE and Log(lambda)"}
cv_model <- cv.glmnet(x, y, alpha = 1)

# find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min

# produce plot of test MSE by lambda value
plot(cv_model)
```

We found that the MSE was minimized when $\lambda$ is equal to: `r best_lambda`

## Model Analysis

#### R-Squared Analysis

```{r}
#calculate R-squared
y_predicted <- predict(model, s = best_lambda, newx =x)

#SST and SSE
sst <- sum((y-mean(y))^2)
sse <- sum((y_predicted - y)^2)

rsq <- 1-(sse/sst)
```

When Lambda equals `r best_lambda`, the R-Squared is `r rsq`. This implies that the model explains approximately`r round(rsq*100,2)`% of the variation in the response in our training data set.

#### Coefficient Analysis


```{r}
temp_lasso <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(temp_lasso)
```

Through observing our coefficients, we can observe that Triples and Home Runs have a much larger effect on the expected number of Runs than any other predictor in a lasso regression model. Intentional walks also decreases the expected number of runs and no predictor can be removed without consequence.

# Comparison of our Models

```{r, echo =FALSE}
batting <- dbGetQuery(ball, "
                            SELECT
                            sum(ab) AS AT_BAT,
                            player_id,
                            sum(r) AS RUNS,
                            sum(hr) AS HOME_RUNS, 
                            sum(triple) AS TRIPLE,
                            sum(double) AS DOUBLE,
                            (sum(h) -  sum(hr) - sum(triple) - sum(double)) AS SINGLES,
                            sum(bb) AS WALKS,
                            sum(ibb) AS INT_WALKS,
                            sum(sb) AS STOLEN_BASES,
                            sum(hbp) AS HIT_BY_PITCH
                            
                            FROM
                            batting
                            where
                            year > 2000
                            group by
                            player_id
                            
                   ")
sumbat <- batting
batting <- subset(batting,batting[,1]>100)

set.seed(4)
batting <- sample_n(batting, 500, replace = FALSE)

players <- dbGetQuery(ball, "
                      
                      SELECT 
                      player_id,
                      (weight / POWER(height, 2)) *703 AS BMI,
                      bats as HAND
                      FROM
                      player
                      
                      ")




players <- subset(players, players[,2]>0)



for ( x in 1:17918)
{
  if(players[x,2] <= 18.5) {players[x,2] <- "U"} 
  else if(players[x,2] <= 24.9) {players[x,2] <- "H"}
  else if(players[x,2] <= 29.9) {players[x,2] <- "O"}
  else {players[x,2] <- "B"}
}

batting <-  merge(batting, players, by="player_id" )

```

```{r}
x <- data.matrix(batting[, c('TRIPLE', 'HOME_RUNS', 'SINGLES', 'WALKS', 'DOUBLE', 'INT_WALKS', 'STOLEN_BASES', 'HIT_BY_PITCH')])
```

```{r}
ridge_model_y_predictions <- data.frame(Prediction = 1:500, Actual= 1:500)
ridge_model_y_predictions[,1] <- predict(ridge_model, s = best_lambda_r, newx = x)
ridge_model_y_predictions[,2] <- batting$RUNS

lasso_model_y_predictions <- data.frame(Prediction = 1:500, Actual = 1:500)
lasso_model_y_predictions[,1] <- predict(cv_model, s = best_lambda, newx = x)
lasso_model_y_predictions[,2] <- batting$RUNS


stat_model_y_predictions <- data.frame(Prediction = 1:500, Actual = 1:500)
stat_model_y_predictions[,1] <- predict(stat_model, newdata = batting)
stat_model_y_predictions[,2] <- batting$RUNS

slm_y_predictions <- data.frame(Prediction = 1:500, Actual = 1:500)
slm_y_predictions[,1] <- predict(slm, newdata = batting)
slm_y_predictions[,2] <- batting$RUNS
```

```{r}
#Rsquare finder
#SLR
#MLR
#Ridge
#Lasso



```

```{r, fig.width=7,fig.height=2, fig.cap="A comparison of the Linear Models"}
ggplot() + 
  
  geom_point(aes(x=Prediction, y=Actual, color="SLM"), slm_y_predictions, size =0.25 )+
  geom_smooth(aes(x=Prediction, y=Actual, color="SLM"), slm_y_predictions, method = lm, fullrange =TRUE, size = 0.75)+
  
  geom_point(aes(x=Prediction, y=Actual, color="MLR"), stat_model_y_predictions,size = 0.25) + 
  geom_smooth(aes(x=Prediction, y=Actual, color="MLR"), stat_model_y_predictions, method = lm, fullrange =TRUE, size = 0.75)+
  
  geom_point(aes(x=Prediction, y=Actual, color="Ridge Model"), ridge_model_y_predictions, size =0.25)+
  geom_smooth(aes(x=Prediction, y=Actual, color="Ridge Model"), ridge_model_y_predictions, method = lm, fullrange =TRUE, size = 0.75)+
  
  geom_point(aes(x=Prediction, y=Actual, color="Lasso Model"), lasso_model_y_predictions, size =0.25 )+
  geom_smooth(aes(x=Prediction, y=Actual, color="Lasso Model"), lasso_model_y_predictions, method = lm, fullrange =TRUE, size = 0.75)+
  

  
  labs(color = "Model")
```

### MSE and $R^2$ By model

| Model Type       |                                                                                                                                                      $R^2$ |                                                                                MSE |
|:------------------------------------------|--------------:|--------------:|
| SLR              |                                 `r 1-(sum((slm_y_predictions[,1]-slm_y_predictions[,2])^2)/sum((slm_y_predictions[,2]-sum(slm_y_predictions[,2])/500)^2))` |                 `r (1/500) * sum((slm_y_predictions[,2]-slm_y_predictions[,1])^2)` |
| MLR              |     `r 1-(sum((stat_model_y_predictions[,1]-stat_model_y_predictions[,2])^2)/sum((stat_model_y_predictions[,2]-sum(stat_model_y_predictions[,2])/500)^2))` |   `r (1/500) * sum((stat_model_y_predictions[,2]-stat_model_y_predictions[,1])^2)` |
| Ridge Regression | `r 1-(sum((ridge_model_y_predictions[,1]-ridge_model_y_predictions[,2])^2)/sum((ridge_model_y_predictions[,2]-sum(ridge_model_y_predictions[,2])/500)^2))` | `r (1/500) * sum((ridge_model_y_predictions[,2]-ridge_model_y_predictions[,1])^2)` |
| Lasso Regression | `r 1-(sum((lasso_model_y_predictions[,1]-lasso_model_y_predictions[,2])^2)/sum((lasso_model_y_predictions[,2]-sum(lasso_model_y_predictions[,2])/500)^2))` | `r (1/500) * sum((lasso_model_y_predictions[,2]-lasso_model_y_predictions[,1])^2)` |

The graph and table above were generated for several different sets of 500 new random observations from our original dataset. In each case, the patterns displayed were consistent with those show above. The Lasso Regression has the lowest Mean Square Error and the Highest $R^2$ of the models--with the Ridge Regression close behind. On the other hand, the Singular linear regression has the lowest $R^2$ and MSE. The Multi-Linear Regression has a slightly lower $R^2$ and MSE than the Lasso and Ridge Regressions. However, it is very close to the shrinkage models and fits the data well--so it might be a good idea to use it if ease of explainability is important.

# Investigation - Principle Component Analysis

```{r, echo =FALSE}
batting <- dbGetQuery(ball, "
                            SELECT
                            sum(ab) AS AT_BAT,
                            player_id,
                            sum(r) AS RUNS,
                            sum(hr) AS HOME_RUNS, 
                            sum(triple) AS TRIPLE,
                            sum(double) AS DOUBLE,
                            (sum(h) -  sum(hr) - sum(triple) - sum(double)) AS SINGLES,
                            sum(bb) AS WALKS,
                            sum(ibb) AS INT_WALKS,
                            sum(sb) AS STOLEN_BASES,
                            sum(hbp) AS HIT_BY_PITCH
                            
                            FROM
                            batting
                            where
                            year > 2000
                            group by
                            player_id
                            
                   ")
sumbat <- batting
batting <- subset(batting,batting[,1]>100)

set.seed(5)
batting <- sample_n(batting, 500, replace = FALSE)

players <- dbGetQuery(ball, "
                      
                      SELECT 
                      player_id,
                      (weight / POWER(height, 2)) *703 AS BMI,
                      bats as HAND
                      FROM
                      player
                      
                      ")




players <- subset(players, players[,2]>0)



for ( x in 1:17918)
{
  if(players[x,2] <= 18.5) {players[x,2] <- "U"} 
  else if(players[x,2] <= 24.9) {players[x,2] <- "H"}
  else if(players[x,2] <= 29.9) {players[x,2] <- "O"}
  else {players[x,2] <- "B"}
}

batting <-  merge(batting, players, by="player_id" )
```

We chose Principal Components Analysis for our innovation because this method is used when there are a large number of predictors. The goal of this method is to replace our predictors with a smaller number of linear combinations of the predictors. We are essentially transforming our data into a lower-dimensional space while collating highly correlated variables together, allowing us to more easily understand and visualize our data. For example, if we have $X_1, X_2,..., X_k$ predictors with k being large or at least $k\geq2$, we want to replace k with $k_0<k$ linear combinations of our predictors.

Let $\mathbf{X'} = (X_1, X_2,...X_k)$ and $\mathbf{u'}$ be a $p\times1$ vector of constants such that $\mathbf{u}_1'\mathbf{u}_1=1$. The first principal component will be the linear combination $Z_1=\mathbf{u'X}$ such that the variance of $Z_1 = \mathbf{u}_1'\text{Var}(\mathbf{X})\mathbf{u}_1$ is as large as possible to retain as much as the variation in the predictors as possible. If $\text{Var}(\mathbf{X})$ is known, then $\mathbf{u}_i$'s are the eigenvectors that corresponds to the $k_0$ largest eigenvalues of $\text{Var}(\mathbf{X})$. If $\text{Var}(\mathbf{X})$ is unknown, like in our case, we replace the variance matrix with the sample covariance matrix.

To find our principal components, first we normalize the data by dividing by the sample standard deviation. Then, we use the sample correlation matrix to compute our eigenvalues ($\hat{\lambda}_j$) and select the the largest ones. Now, we compute the corresponding eigenvectors ($\mathbf{\hat{u}}_j$) and multiply them by $\mathbf{X'}$. These are our principal components $Z_j$.

```{r include =FALSE}
# Removing unnecessary columns
numerical_data <- batting[,4:11]

# Data normalization
data_normalized <- scale(numerical_data)

# PCA computation - computing our eigenvalues
data.pca <- princomp(data_normalized)
summary(data.pca) # components one and two have the highest variance

# Loading matrix
data.pca$loadings[, 1:2] # weights of component one are pretty evenly spread, while component two puts more weight on hit by pitch and triples
```

### Visualizations

```{r, fig.cap = "Component contributions to the total variance"}
# Scree plot - importance of each component//contribution to total variance
fviz_eig(data.pca, addlabels=TRUE)
```

This plot displays the eigenvalues and shows us that the first two components contribute the most the total variance.

```{r, include = FALSE}
# Biplot of the attributes - visualize similarities between samples and see impact of each attribute on components
fviz_pca_var(data.pca, col.var="black")

# Contribution of each variable - how much each variable is represented in a given component (utilizing square cosine) 
# High value means good representation
fviz_cos2(data.pca, choice="var", axes=1:2)
```

```{r fig.cap = "Biplot combined with cos2"}
# Biplot combined with cos2
fviz_pca_var(data.pca, col.var = "cos2",
            gradient.cols = c("black", "orange", "green"),
            repel = TRUE)
```

In this plot, predictors that are grouped together are correlated to each other. In our case, home runs, intentional walks, hit by pitches, walks, doubles and singles are positively correlated together in both components. Additionally, stolen bases and triples are correlated to each other positively in component one and negatively in component two. Finally, from this plot we can see that predictors that are farther from the origin and closer to green (representing a high square cosine) are better represented. For our predictors, they are all represented fairly similarly, with doubles being the most and hit by pitches and intentional walks falling behind.

### Analysis

By looking at our calculations and visualizations, we can see that the first two components capture almost all of the variance in our predictors, so we would select these for our model. The weights of component one are fairly evenly distributed over the predictors while component two puts more weight on stolen bases and triples.

### Problems that Could Arise

If the data is not a random sample from the population, then the variables will be measured on some arbitrary scale that depends on the sampling design. This is because the sample standard deviations used to standardize the variables will not align with the population. Our sample is a random sample from the population, so we do not run into this issue.

# Conclusion

If we want to include as many of our predictors as possible, there are issues with collinearity among some predictors. To mitigate this issue, a Ridge or Lasso regression can be employed, with the Lasso regression with a $\lambda$ value equal to `r best_lambda` being the best fit. However, for ease of explanation one could consider using the Multi-Linear Regression we created in step 3 as it has much fewer predictors and the fit and predictive accuracy of the model is very similar to the models generated through Shrinkage methods. In our innovative step, we used principal component analysis to demonstrate that we can take linear combinations of our predictors to decrease our number of regressors to two while still getting a well fit model.
